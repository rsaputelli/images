import streamlit as st
import requests
from urllib.parse import urljoin, urlparse
from urllib import robotparser
from bs4 import BeautifulSoup
import re
import pandas as pd
import tldextract
import time
from io import BytesIO
import hashlib
from concurrent.futures import ThreadPoolExecutor, as_completed
from PIL import Image, UnidentifiedImageError

# ==========================
# App Config & Constants
# ==========================
st.set_page_config(page_title="Website Image Licensing Audit (MVP)", layout="wide")
st.title("🕵️ Website Image Licensing Audit — MVP")

DEFAULT_HEADERS = {
    "User-Agent": "ImageLicenseAuditor/1.0 (+https://example.org; contact=webmaster@example.org)"
}

STOCK_DOMAINS = {
    "shutterstock.com", "gettyimages.com", "istockphoto.com", "adobestock.com", "stock.adobe.com",
    "depositphotos.com", "dreamstime.com", "alamy.com", "123rf.com", "bigstockphoto.com",
    "canstockphoto.com", "pond5.com", "pixabay.com", "unsplash.com", "pexels.com"
}

IMG_EXTS = {".jpg", ".jpeg", ".png", ".gif", ".webp", ".svg"}
URL_IN_CSS = re.compile(r"url\(([^)]+)\)")

# ==========================
# Sidebar Controls
# ==========================
with st.sidebar:
    st.header("Settings")
    start_url = st.text_input("Start URL", placeholder="https://example.com")

    st.markdown("**Scope & Depth**")
    include_subdomains = st.checkbox("Include subdomains", value=False)
    depth_max = st.slider("Max crawl depth", min_value=1, max_value=5, value=3)

    st.markdown("**Limits**")
    max_pages_default = 100
    max_images_default = 1000
    per_page_img_cap_default = 50
    per_image_size_mb_default = 5
    total_bytes_cap_mb_default = 200

    max_pages = st.slider("Max pages", 10, 300, max_pages_default, step=10)
    max_images = st.slider("Max images (total)", 100, 2000, max_images_default, step=100)
    per_page_img_cap = st.slider("Per-page image cap", 10, 100, per_page_img_cap_default, step=5)
    per_image_size_mb = st.slider("Per-image size cap (MB)", 1, 20, per_image_size_mb_default)
    total_bytes_cap_mb = st.slider("Total download cap (MB)", 50, 400, total_bytes_cap_mb_default, step=25)

    st.markdown("**Fetch Policy**")
    concurrency = st.slider("Concurrency (workers)", 1, 10, 5)
    base_delay_ms = st.slider("Base delay between requests (ms)", 0, 1000, 300, step=50)

    st.markdown("**Features**")
    parse_css_backgrounds = st.checkbox("Capture CSS background images", value=True)
    try_exif = st.checkbox("Attempt EXIF/IPTC (≤ size cap)", value=True)
    show_thumbs = st.checkbox("Show thumbnails (may be slow)", value=False)

    st.markdown("**Power User**")
    power = st.checkbox("Enable power-user mode (lifts caps, use cautiously)", value=False)
    if power:
        st.info("Power user mode enabled. Be respectful of target sites and your Streamlit resource limits.")
        # Lift some caps visually (actual caps governed by the sliders above)

    go = st.button("Run Audit", type="primary")
    stop = st.button("Stop")

# Stop flag
if "_stop" not in st.session_state:
    st.session_state._stop = False
if stop:
    st.session_state._stop = True

# ==========================
# Helpers
# ==========================

def normalized(url: str) -> str:
    try:
        u = url.strip().strip('"\'')
        return u
    except Exception:
        return url


def same_scope(url: str, root: str, include_subs: bool) -> bool:
    try:
        t_root = tldextract.extract(root)
        t_url = tldextract.extract(url)
        # Must match registrable domain and suffix
        same_reg = (t_root.domain == t_url.domain and t_root.suffix == t_url.suffix)
        if not same_reg:
            return False
        if include_subs:
            return True
        # Without subdomains: either exact host match or both are bare
        return (t_root.subdomain == t_url.subdomain)
    except Exception:
        return False


def get_robots_session(base_url: str):
    parsed = urlparse(base_url)
    robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
    rp = robotparser.RobotFileParser()
    try:
        rp.set_url(robots_url)
        rp.read()
    except Exception:
        pass
    s = requests.Session()
    s.headers.update(DEFAULT_HEADERS)
    return rp, s


def polite_get(session: requests.Session, url: str, delay_ms: int, timeout: int = 15):
    time.sleep(delay_ms / 1000.0)
    try:
        resp = session.get(url, timeout=timeout)
        return resp
    except Exception:
        return None


def extract_img_links_from_html(base_url: str, html: str, per_page_cap: int):
    soup = BeautifulSoup(html, 'html.parser')
    found = []
    # IMG tags
    for img in soup.find_all('img'):
        src = img.get('src') or img.get('data-src')
        if not src:
            continue
        u = urljoin(base_url, src)
        alt = img.get('alt') or ''
        found.append((u, 'IMG Tag', alt))
        if len(found) >= per_page_cap:
            break

    # Inline style backgrounds
    for el in soup.find_all(style=True):
        style = el.get('style')
        for m in URL_IN_CSS.findall(style or ''):
            u = m.strip('"\'')
            u = urljoin(base_url, u)
            found.append((u, 'CSS Background', ''))
            if len(found) >= per_page_cap:
                break
        if len(found) >= per_page_cap:
            break

    # Linked CSS files
    css_links = []
    for link in soup.find_all('link', rel=lambda x: x and 'stylesheet' in x):
        href = link.get('href')
        if href:
            css_links.append(urljoin(base_url, href))

    return found, css_links


def extract_urls_from_css(css_text: str, base_url: str):
    urls = []
    for m in URL_IN_CSS.findall(css_text or ''):
        u = m.strip('"\'')
        u = urljoin(base_url, u)
        urls.append(u)
    return urls


def file_ext(url: str) -> str:
    path = urlparse(url).path.lower()
    for ext in IMG_EXTS:
        if path.endswith(ext):
            return ext
    return ''


def domain_of(url: str) -> str:
    try:
        return tldextract.extract(url).registered_domain
    except Exception:
        return ''


def guessed_source(url: str) -> str:
    dom = domain_of(url)
    if dom in STOCK_DOMAINS:
        return f"Stock / Library ({dom})"
    return "Unknown / Site-hosted"


def reverse_links(url: str):
    g = f"https://www.google.com/searchbyimage?image_url={requests.utils.quote(url, safe='')}"
    t = f"https://tineye.com/search?url={requests.utils.quote(url, safe='')}"
    return g, t


def head_size(session: requests.Session, url: str, timeout: int = 15):
    try:
        r = session.head(url, allow_redirects=True, timeout=timeout)
        size = r.headers.get('Content-Length')
        ct = r.headers.get('Content-Type', '')
        return int(size) if size and size.isdigit() else None, ct
    except Exception:
        return None, ''


def fetch_bytes(session: requests.Session, url: str, max_bytes: int):
    try:
        r = session.get(url, stream=True, timeout=20)
        r.raise_for_status()
        buf = BytesIO()
        total = 0
        for chunk in r.iter_content(8192):
            if chunk:
                buf.write(chunk)
                total += len(chunk)
                if total > max_bytes:
                    break
        buf.seek(0)
        return buf, total
    except Exception:
        return None, 0


def try_make_thumb(img_bytes: BytesIO):
    try:
        with Image.open(img_bytes) as im:
            im.thumbnail((128, 128))
            out = BytesIO()
            im.save(out, format='PNG')
            out.seek(0)
            return out
    except (UnidentifiedImageError, OSError):
        return None

# ==========================
# Main Audit Logic
# ==========================

if go:
    st.session_state._stop = False

    if not start_url:
        st.error("Please enter a start URL.")
        st.stop()

    rp, session = get_robots_session(start_url)

    parsed_root = urlparse(start_url)
    base_origin = f"{parsed_root.scheme}://{parsed_root.netloc}"

    # Robots check for start page
    if rp and not rp.can_fetch(DEFAULT_HEADERS["User-Agent"], start_url):
        st.warning("robots.txt disallows crawling the start URL. Aborting.")
        st.stop()

    q = [start_url]
    visited_pages = set()
    depth = {start_url: 0}

    pages_processed = 0
    images_found = 0
    total_bytes_downloaded = 0

    rows = []
    css_queue = []

    progress = st.progress(0)
    status = st.empty()

    while q and not st.session_state._stop:
        if pages_processed >= max_pages:
            status.info("Hit page limit. Stopping crawl.")
            break

        url = q.pop(0)
        if url in visited_pages:
            continue
        visited_pages.add(url)
        d = depth.get(url, 0)

        if d > depth_max:
            continue

        if rp and not rp.can_fetch(DEFAULT_HEADERS["User-Agent"], url):
            continue

        # Fetch page
        resp = polite_get(session, url, base_delay_ms)
        if not resp or not (200 <= resp.status_code < 300):
            continue

        content_type = resp.headers.get('Content-Type', '')
        if 'text/html' not in content_type:
            continue

        html = resp.text
        page_imgs, css_links = extract_img_links_from_html(url, html, per_page_img_cap)

        # Add CSS links to queue for later fetch
        if parse_css_backgrounds:
            for css in css_links:
                if same_scope(css, start_url, include_subdomains):
                    css_queue.append((url, css))

        # Discover links for crawl (same-scope only)
        soup = BeautifulSoup(html, 'html.parser')
        for a in soup.find_all('a', href=True):
            href = urljoin(url, a['href'])
            # Clean fragment
            href = href.split('#')[0]
            if not same_scope(href, start_url, include_subdomains):
                continue
            if href not in visited_pages and href not in depth:
                depth[href] = d + 1
                if d + 1 <= depth_max:
                    q.append(href)

        # Process page images
        # We will HEAD concurrently to estimate sizes/content-types
        to_process = list({u for (u, _, _) in page_imgs if file_ext(u) or True})  # keep all; ext check later
        results = []
        with ThreadPoolExecutor(max_workers=concurrency) as ex:
            futs = {ex.submit(head_size, session, u): u for u in to_process}
            for fut in as_completed(futs):
                u = futs[fut]
                size, ct = fut.result()
                results.append((u, size, ct))

        # Build rows
        for (u, stype, alt) in page_imgs:
            if images_found >= max_images:
                break
            if st.session_state._stop:
                break

            ext = file_ext(u)
            dom = domain_of(u)
            source_guess = guessed_source(u)
            g_link, t_link = reverse_links(u)

            # Size from HEAD (if available)
            match = next((r for r in results if r[0] == u), None)
            est_bytes = match[1] if match else None
            content_type = match[2] if match else ''

            size_ok = True
            if est_bytes is not None and est_bytes > per_image_size_mb * 1024 * 1024:
                size_ok = False
                note = "Skipped (exceeds per-image size cap)"
            else:
                note = ""

            thumb_b64 = None
            exif_author = ""

            # Optional EXIF + thumbnail if size OK and content looks like an image
            if try_exif and size_ok and (content_type.startswith("image/") or ext in IMG_EXTS):
                # Respect total bytes cap
                if total_bytes_downloaded < total_bytes_cap_mb * 1024 * 1024:
                    # Download up to per-image cap
                    buf, n = fetch_bytes(session, u, per_image_size_mb * 1024 * 1024)
                    total_bytes_downloaded += n
                    if buf:
                        # EXIF
                        try:
                            with Image.open(buf) as im:
                                exif = im.getexif()
                                if exif:
                                    # Common Artist tag 315
                                    artist = exif.get(315)
                                    if artist:
                                        exif_author = str(artist)
                        except (UnidentifiedImageError, OSError):
                            pass
                        # Thumbnail if requested
                        if show_thumbs:
                            thumb = try_make_thumb(buf)
                            if thumb:
                                # Streamlit can show image directly later
                                pass
                else:
                    note = (note + "; " if note else "") + "Skipped (hit total download cap)"

            risk = []
            if domain_of(u) in STOCK_DOMAINS:
                risk.append("Stock source — ensure license")
            if not alt:
                risk.append("No alt text (check provenance)")

            rows.append({
                "Page": url,
                "Image URL": u,
                "Source Type": stype,
                "Alt Text": alt,
                "Domain": dom,
                "Guessed Source": source_guess,
                "Content-Type": content_type,
                "Estimated Bytes": est_bytes,
                "EXIF Artist": exif_author,
                "Google Images": g_link,
                "TinEye": t_link,
                "Notes": note,
                "Risk Flags": ", ".join(risk)
            })
            images_found += 1

        pages_processed += 1
        progress.progress(min(1.0, pages_processed / max(1, max_pages)))
        status.write(f"Crawled {pages_processed} page(s), collected {images_found} image(s).")

        # Early exit if image cap reached
        if images_found >= max_images:
            status.info("Hit image limit. Stopping crawl.")
            break

        # Fetch linked CSS files (lightweight)
        if parse_css_backgrounds and css_queue and not st.session_state._stop:
            new_css_queue = []
            with ThreadPoolExecutor(max_workers=min(concurrency, 4)) as ex:
                futs = {ex.submit(polite_get, session, css_url, base_delay_ms): (page_url, css_url)
                        for (page_url, css_url) in css_queue}
                for fut in as_completed(futs):
                    page_url, css_url = futs[fut]
                    resp_css = fut.result()
                    if not resp_css or resp_css.status_code != 200:
                        continue
                    css_text = resp_css.text
                    for u in extract_urls_from_css(css_text, css_url):
                        if images_found >= max_images:
                            break
                        ext = file_ext(u)
                        if ext == '':
                            # keep anyway; might be image without extension
                            pass
                        # Use HEAD to evaluate size/content-type
                        size, ct = head_size(session, u)
                        size_ok = True
                        note = ""
                        if size is not None and size > per_image_size_mb * 1024 * 1024:
                            size_ok = False
                            note = "Skipped (exceeds per-image size cap)"
                        exif_author = ""
                        if try_exif and size_ok and (ct.startswith("image/") or ext in IMG_EXTS):
                            if total_bytes_downloaded < total_bytes_cap_mb * 1024 * 1024:
                                buf, n = fetch_bytes(session, u, per_image_size_mb * 1024 * 1024)
                                total_bytes_downloaded += n
                                if buf:
                                    try:
                                        with Image.open(buf) as im:
                                            exif = im.getexif()
                                            if exif:
                                                artist = exif.get(315)
                                                if artist:
                                                    exif_author = str(artist)
                                    except (UnidentifiedImageError, OSError):
                                        pass
                            else:
                                note = (note + "; " if note else "") + "Skipped (hit total download cap)"

                        g_link, t_link = reverse_links(u)
                        dom = domain_of(u)
                        rows.append({
                            "Page": page_url,
                            "Image URL": u,
                            "Source Type": "CSS Background",
                            "Alt Text": "",
                            "Domain": dom,
                            "Guessed Source": guessed_source(u),
                            "Content-Type": ct,
                            "Estimated Bytes": size,
                            "EXIF Artist": exif_author,
                            "Google Images": g_link,
                            "TinEye": t_link,
                            "Notes": note,
                            "Risk Flags": "Stock source — ensure license" if dom in STOCK_DOMAINS else ""
                        })
                        images_found += 1
                        if images_found >= max_images:
                            break
            css_queue = new_css_queue

    # ==========================
    # Results & Export
    # ==========================
    if rows:
        df = pd.DataFrame(rows)
        st.success(f"Audit complete: {pages_processed} page(s), {images_found} image(s).")

        # Quick filters
        col1, col2, col3 = st.columns([1,1,2])
        with col1:
            only_stock = st.checkbox("Show likely stock/library only", value=False)
        with col2:
            only_risky = st.checkbox("Show rows with risk flags", value=False)
        with col3:
            search = st.text_input("Search URL/Alt/Domain contains…", value="")

        view = df.copy()
        if only_stock:
            view = view[view['Guessed Source'].str.contains('Stock / Library', na=False)]
        if only_risky:
            view = view[view['Risk Flags'].str.len() > 0]
        if search:
            mask = (
                view['Image URL'].str.contains(search, case=False, na=False) |
                view['Page'].str.contains(search, case=False, na=False) |
                view['Alt Text'].str.contains(search, case=False, na=False) |
                view['Domain'].str.contains(search, case=False, na=False)
            )
            view = view[mask]

        st.dataframe(view, use_container_width=True, hide_index=True)

        # Export buttons
        def to_excel_bytes(df_: pd.DataFrame) -> bytes:
            out = BytesIO()
            with pd.ExcelWriter(out, engine="xlsxwriter") as writer:
                df_.to_excel(writer, sheet_name="Audit", index=False)
            out.seek(0)
            return out.read()

        csv_bytes = view.to_csv(index=False).encode('utf-8')
        xlsx_bytes = to_excel_bytes(view)

        st.download_button("Download CSV", data=csv_bytes, file_name="image_licensing_audit.csv", mime="text/csv")
        st.download_button("Download Excel", data=xlsx_bytes, file_name="image_licensing_audit.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

        st.caption("Notes: Respect each site's robots.txt and terms. Reverse-image links open Google Images/TinEye with the image URL prefilled; results are not scraped.")
    else:
        st.warning("No images found or crawl blocked. Try adjusting limits, enabling subdomains, or verifying the start URL.")
